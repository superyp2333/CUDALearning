# CUDA 学习

## 一、仓库管理

```bash
# 初始化本地库
git init
# 本地库与远程库建立关联：仓库别名默认为origin
git remote add origin git@github.com:superyp2333/CUDALearning.git
git remote -v

# 将修改添加到暂存区跟踪
git add .
# 提交暂存区，形成提交记录
git commit -m "第一次提交"

# 修改上次提交
git add .
# 只合并代码修改，「保留原来的提交备注」（推荐）
git commit --amend --no-edit
# 合并代码修改 + 「同时修改提交备注」（按需使用）
git commit --amend -m "balabala"

# 拉取最新代码：单人用pull「拉取+合并，会直接报冲突」，多人用fetch「只拉取，不合并，需要手动合并」
git pull
git fetch

# 首次推送需要加 -u，将「本地的main分支」与「远程origin仓库的main分支」建立关联
git push -u origin master
# 建立起关联后，后续推送可简化
git push
```



## 二、GPU环境准备

### 2.1 Kaggle

🚀 免费的GPU环境：[Kaggle](https://www.kaggle.com/)

<img src="./assets/image-20260110171544563.png" alt="image-20260110171544563" />

⚠️ 注意：手动关闭后台任务，否则GPU一直在占用，很快就把每周30小时的GPU额度用完了

<img src="./assets/image-20260110172228083.png" alt="image-20260110172228083.png" width="60%" />

✅ 查看每周剩余的GPU额度：头像 -> settings -> Quotas

<img src="./assets/image-20260111144127561.png" alt="image-20260111144127561" width="60%" />

### 2.2 CUDA C/Cpp

**在 Kaggle Notebook 中运行 CUDA C/Cpp 代码，需要在环境中安装一个CUDA运行插件，否则无法编译运行**

✅ 1. 验证 GPU 是否配置正确、验证 nvcc 编译器是否安装

```bash
# 查询当前机器的 GPU 硬件信息 + 显卡驱动版本 + CUDA 驱动 API 版本 + 显存占用 / 算力状态
!nvidia-smi

# 查看 CUDA C/Cpp 编译器版本
!nvcc -v
```

✅ 2. 安装 `nvcc4jupyter` 插件

```bash
# 安装插件
!pip install nvcc4jupyter

# 加载插件，让该环境支持直接编译、运行 CUDA C/Cpp 代码；在运行 CUDA 代码时，必须在代码前加上魔法命令：%%cuda
%load_ext nvcc4jupyter
```

✅ 3. 编译和运行 CUDA C/Cpp 代码

```cpp
%%cuda
#include <iostream>
using namespace std;

// GPU端 CUDA核函数：最简单的加法运算
__global__ void simple_add(int *a, int *b, int *c)
{
    *c = *a + *b;
}

int main()
{
    // CPU 端 变量 (主机端 Host)
    int a = 10, b = 20, res;
    // GPU 端 指针 (设备端 Device)
    int *d_a, *d_b, *d_c;

    // 1. 给GPU分配显存
    cudaMalloc(&d_a, sizeof(int));
    cudaMalloc(&d_b, sizeof(int));
    cudaMalloc(&d_c, sizeof(int));

    // 2. CPU数据 拷贝到 GPU显存
    cudaMemcpy(d_a, &a, sizeof(int), cudaMemcpyHostToDevice);
    cudaMemcpy(d_b, &b, sizeof(int), cudaMemcpyHostToDevice);

    // 3. 启动GPU核函数：<<<线程块数, 每个线程块的线程数>>>
    simple_add<<<1, 1>>>(d_a, d_b, d_c);

    // 4. GPU计算结果 拷贝回 CPU内存
    cudaMemcpy(&res, d_c, sizeof(int), cudaMemcpyDeviceToHost);

    // 打印结果
    cout << "CUDA验证成功！计算结果：" << a << " + " << b << " = " << res << endl;

    // 释放GPU显存（好习惯）
    cudaFree(d_a);
    cudaFree(d_b);
    cudaFree(d_c);

    return 0;
}
```

安装插件之后，可以调用底层的 nvcc 编译器，「编译」+「运行」单元格中的代码

<img src="./assets/image-20260111160415399.png" alt="image-20260111160415399" width="60%"/>



✨ Tips：正常的编译和运行过程如下

```bash
# 假设 CUDA 代码写在 main.cu 中，则如下命令可以编译生成一个文件名为 main 的可执行文件
nvcc main.cu -o main

# 运行 CUDA 代码
./main
```



## 三、CUDA编程入门

### 3.1 GPU 的基本架构

> **GPU 芯片** → 包含多个 **SM 处理器** → 每个 SM 包含多个 **流处理器（CUDA 核心）** + 缓存 + 线程调度器 + 指令单元

✅ GPU 实际上是由多个「SM」(Streaming Multiprocessor，流多处理器) 组成的阵列

* 「SM」内包含数十个「SP」(Streaming Processors, 流处理器），以及配套的线程调度器、共享内存、指令单元等资源
* 「SP」也称为 CUDA 核心，**它的本质是单指令计算核心：一个核心在某个时刻，只能执行一条指令（比如加减乘除、浮点运算）**



🚀 **以 Tesla P100 为例**

已知它共有 3584 个核心，每个 「SM」共 64 个「SP」，因此它共有 3584/64 = 56 个「SM」

也就说 Tesla P100 芯片，是由 **56 个独立的 SM 计算集群** 组成的，每个集群又包含 64 个流处理器，再加上配套的缓存和调度器

| SM 内部组件                    | 数量（每 SM） | 作用                                                         |
| ------------------------------ | ------------- | ------------------------------------------------------------ |
| 流处理器（CUDA 核心）          | 64 个         | 执行核心计算任务（加减乘除、矩阵运算等）                     |
| 双精度浮点单元（FP64）         | 32 个         | 专门处理高精度浮点数计算（科学计算 / 深度学习常用）          |
| 共享内存（Shared Memory）      | 64 KB         | SM 内所有流处理器共享的高速缓存，比显存快 10~100 倍          |
| 线程束调度器（Warp Scheduler） | 2 个          | 负责调度 SM 内的线程束（32 个线程为一束），分配到流处理器执行 |
| 指令单元（Instruction Unit）   | 2 个          | 解码 CUDA 指令，分发给流处理器                               |



✨ **概念等价性：流处理器 = GPU 核心 = CUDA核心**

* 「流处理器」是硬件领域的通用叫法
* 「GPU 核心」是用户层面的通俗叫法
* 「CUDA 核心」是 NVIDIA 针对 CUDA 编程的专属叫法
* 三者都指向同一个硬件电路模块 —— 你可以理解为：**GPU 的最小计算「工人」**



✨ **「SM」与 「SP」的关系**

用「军队编制」的类比，能把两者的关系讲得最清楚

| 硬件层级                            | 类比军队编制   | 作用                                                   | 你的 P100 配置                                               |
| ----------------------------------- | -------------- | ------------------------------------------------------ | ------------------------------------------------------------ |
| GPU 芯片                            | 整个军团       | 独立的计算硬件实体                                     | Tesla P100 芯片                                              |
| SM                                  | 一个「作战营」 | GPU 的基本调度单位，CUDA 驱动 **按「SM」分配任务**     | 共 56 个作战营                                               |
| SP（CUDA 核心）                     | 营里的「士兵」 | 最小的计算执行者，士兵归营长「SM」管理                 | 每个营 64 名士兵 → 总计 3584 名                              |
| Warp（线程束，每 32 个线程为 1 束） | 32个任务       | 32个任务（相同指令，不同数据），需要 32 个士兵同时来干 | 32个线程                                                     |
| Warp Scheduler（线程束调度器）      | 排长           | 不执行任务，只负责分配任务给士兵                       | 每个 SM 里有 **2 个线程束调度器** -> 相当于一个作战营里有两个分配任务的排长 |

⚠️ 注意：线程束调度器调度的**32 个线程是绑定在一起的** —— 它们必须同时执行**完全相同的指令**（比如同时给数组元素乘 2）



✨ **「核心」与「线程」的关系**

* 核心 (Core)-> 硬件物理实体，真实的「计算单元」
  * 核心的数量是**出厂固定的**，比如 Tesla P100 有 3584 个核心，永远不会变
  * 核心的唯一作用：**执行计算指令**（加减乘除、逻辑运算等），一个核心在一个时刻只能执行「一个基础计算指令」
  * 核心是「算力的硬件载体」，核心越多，理论算力越高
* 线程 (Thread) -> 软件逻辑实体，虚拟的「执行任务」
  * 一个线程 =「一段要被执行的代码逻辑」，因此线程的数量是**软件层面动态创建的**，可以无限多（受内存限制），比如 CUDA 核函数可以启动 1000/10000/100万个线程
  * 线程的唯一作用：**承载一段要执行的代码**，线程需要「绑定到核心上」才能被执行，线程本身不能产生算力
  * 线程是「任务的软件载体」，线程越多，代表被并行执行的任务越多



✨ **线程如何绑定到核心上？**

* 第一步：CPU 总指挥 -> 下达任务，创建 100 万个线程
* 第二步：CUDA 驱动会把这 100 万个线程，按我指定的规格（`256线程/块`），拆分成 3907 个线程块（1000000/256≈3907），因此每个线程块 = 8 个 Warp（256÷32=8），然后驱动会把这 3907 个线程块，均匀分配给 P100 的 **56 个 SM 作战营**
* 第三步：每个 SM 作战营收到 N 个线程块，营里有 2 个调度队长会把每个线程块拆分成 8 个 warp（256/32=8），所以整个营一共有 8N 个 warp
* 第四步：这 2 个调度队长每次可以派发 2 个 Warp 给营里的士兵，让他们同时来干。因此每个士兵每次可以领到一个线程，干完了再领下一个线程，永远有活干，不会闲置
  * 1 个 Warp 需要 32 个 SP 并行执行，2 个 Warp 就可以喂饱整个 SM 共 64 个 SP，每个 SP 干完了手中的线程又会被 Warp Scheduler 分配到下一个线程，保证了每个 SP 都是满负荷运转
  * 因此 GPU 以极小的硬件代价，在每个 SM 中设置了 2 个 Warp Scheduler，实现了让 64 个 SP 的利用率将近 100%，性价比之王！



### 3.2 异构计算架构

✅ **什么是「CPU+GPU 异构计算」？**

✔️ 通俗大白话定义

> CPU+GPU 异构计算 = **让 CPU 和 GPU「分工协作、各尽其能」完成同一个计算任务**，把合适的代码交给合适的硬件去跑，最终实现「1+1 >> 2」的极致加速效果。



✅ **为什么需要 CPU+GPU 异构计算？**

✔️ CPU 和 GPU 的「硬件架构 + 算力特点」核心对比

| 硬件 | 核心架构特点                                                 | 算力优势                                                     | 算力短板                                                 |
| :--- | :----------------------------------------------------------- | :----------------------------------------------------------- | :------------------------------------------------------- |
| CPU  | 「少核心 + 强单核」，一般 8/16/32 核心，核心主频高、缓存大、逻辑控制能力极强 | 擅长**复杂逻辑判断、串行计算、分支跳转、任务调度**           | 并行计算能力弱，算力天花板低，处理大规模数据循环速度极慢 |
| GPU  | 「超多核心 + 弱单核」，Tesla P100 拥有 **3584 个流处理器 (核心)**，核心主频低，但核心数量是 CPU 的几百倍 | 擅长**无脑并行计算、无分支的海量数据循环、矩阵运算、向量运算**，算力是 CPU 的**50~200 倍** | 逻辑判断能力弱，分支多的代码会大幅降速，单核心执行效率低 |

🔥 一句话总结

>**CPU 是「大脑 / 指挥官」**：负责统筹全局、发号施令、处理复杂逻辑、读写文件
>
>**GPU 是「千军万马 / 苦力大军」**：负责执行海量、重复、简单的并行计算任务
>
>异构计算 = CPU 指挥，GPU 干活，完美分工！



✅ **CPU+GPU 异构计算的「标准工作流程」（所有 CUDA 代码都遵循这个流程）**

<img src="./assets/image-20260111161452057.png" alt="image-20260111161452057" width="80%"/>

⚠️ 前置说明：

* CPU 管理的内存叫「**主机内存 (Host Memory)**」，GPU 管理的内存叫「**设备内存 (Device Memory)**，也叫显存」

* 两者物理隔离、互不互通，数据传输经过**「PCIe 高速总线」**进行拷贝

<img src="./assets/image-20260111174144037.png" alt="image-20260111174144037" width="90%" />



**1、CPU 初始化任务（CPU 干活）**

* CPU 负责：定义计算任务、初始化「主机内存」**（变量/数组/矩阵、读取数据集）**、设置计算参数**（数据量大小、线程块配置）**

**2、CPU 给 GPU 分配显存（CPU 发指令，GPU 执行）**

* CPU 调用 CUDA API `cudaMalloc(&设备指针, 内存大小)`

* 作用：向 GPU 申请一块「设备内存」，用来存放 GPU 要计算的数据，该内存**只有 GPU 能访问**，CPU 不能直接读写

  > 对于我：Tesla P100 显卡拥有 16G 显存，我就可以申请超大数组（比如千万级 float 数组）

**3、CPU → GPU 数据拷贝（CPU 发指令，硬件执行）**

* CPU 调用 CUDA API：`cudaMemcpy(设备指针, 主机指针, 内存大小, cudaMemcpyHostToDevice)`

* 作用：把「CPU 主机内存」中的「待计算数据」，通过 PCIe 高速总线，拷贝到「GPU 设备内存」中

* 核心注意：**「数据搬运环节」是异构计算中唯一的性能损耗点**，但对于大规模计算，这点损耗完全可以忽略（计算收益远大于拷贝损耗）

  > 对于我：Tesla P100 显卡的 PCIe 接口，带宽充足，数据拷贝速度极快，不需要优化

**4、CPU 启动 GPU 核函数，执行并行计算（CPU 发指令，GPU 满负荷干活）**

* CPU 调用 CUDA 核函数：`核函数名<<<线程块数, 线程数>>>(GPU参数列表)`

* 作用：CPU 下达计算指令，GPU 立刻启动**成百上千个线程**，并行执行核函数中的计算逻辑，此时 GPU 的利用率会瞬间飙升到 50%~100%（可以用`nvidia-smi`看到`GPU-Util`上涨）

* 核心语法：`<<<grid, block>>>` 是 CUDA 的「线程配置语法」，专门用来告诉 GPU 用多少线程来并行计算

  > 对于我：Tesla P100 支持超大线程配置，轻松并行处理百万级数据，算力拉满！

**5、GPU → CPU 结果拷贝（CPU 发指令，硬件执行）**

* CPU 调用 CUDA API：`cudaMemcpy(主机指针, 设备指针, 内存大小, cudaMemcpyDeviceToHost)`
* 作用：把 GPU 计算完成的「结果数据」，从 「GPU 设备内存」拷贝回 「CPU 主机内存」

**6、释放内存 + 收尾工作（CPU 干活）**

* CPU 调用 CUDA API：`cudaFree(设备指针)` 释放 GPU 显存；调用 `free/delete` 释放 CPU 内存
* 避免内存泄漏，尤其是 GPU 显存 不释放会一直占用（`nvidia-smi`能看到显存占用不回落）
